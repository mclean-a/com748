{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import dask.dataframe as ddf\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the dataset: Using the columns identified in null analysis and selecting only specific Seagate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_files = glob.glob('C:/com748/data/raw/parquet/*.parquet')\n",
    "\n",
    "for file in parquet_files:\n",
    "\n",
    "    date = Path(file).stem\n",
    "\n",
    "    # Get columns with low null percentage\n",
    "    df = pd.read_parquet(file, \n",
    "                            columns=['date', 'serial_number', 'model', 'capacity_bytes', 'failure', 'smart_1_normalized', 'smart_1_raw', \n",
    "                                   'smart_3_normalized', 'smart_3_raw', 'smart_4_normalized', 'smart_4_raw', 'smart_5_normalized', 'smart_5_raw', \n",
    "                                   'smart_7_normalized', 'smart_7_raw', 'smart_9_normalized', 'smart_9_raw', 'smart_10_normalized', 'smart_10_raw', \n",
    "                                   'smart_12_normalized', 'smart_12_raw', 'smart_187_normalized', 'smart_187_raw', 'smart_188_normalized', 'smart_188_raw', \n",
    "                                   'smart_190_normalized', 'smart_190_raw', 'smart_192_normalized', 'smart_192_raw', 'smart_193_normalized', 'smart_193_raw', \n",
    "                                   'smart_194_normalized', 'smart_194_raw', 'smart_197_normalized', 'smart_197_raw', 'smart_198_normalized', 'smart_198_raw', \n",
    "                                   'smart_199_normalized', 'smart_199_raw', 'smart_240_normalized', 'smart_240_raw', 'smart_241_normalized', 'smart_241_raw', \n",
    "                                   'smart_242_normalized', 'smart_242_raw'], \n",
    "                            engine='pyarrow')\n",
    "    \n",
    "    df = df[df['model'].isin(['ST4000DM000', 'ST12000NM0007', 'ST8000NM0055', 'ST3000DM001', 'ST12000NM0008', 'ST8000DM002', 'ST14000NM001G'])]\n",
    "\n",
    "    parquet_path = f'C:/com748/data/processed/daily/{date}.parquet'\n",
    "    table_data = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table_data, parquet_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect failure SMART data for various lookahead windows (days):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day minus 0 (last day drive was operational before failing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ali_m\\AppData\\Local\\Temp\\ipykernel_43912\\920707536.py:15: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  output_df = pd.concat(dataframes_list, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Day minus 0 (last day drive was operational before failing)\n",
    "parquet_files = glob.glob('C:/com748/data/processed/daily/*.parquet')\n",
    "\n",
    "dataframes_list = []\n",
    "\n",
    "for file in reversed(parquet_files):\n",
    "    date = Path(file).stem\n",
    "    df = ddf.read_parquet(file, ignore_metadata_file=True, engine='pyarrow')\n",
    "\n",
    "    df_failures = df[df['failure'] == 1]\n",
    "    df_failures['file_date'] = date\n",
    "\n",
    "    dataframes_list.append(df_failures.compute())\n",
    "\n",
    "output_df = pd.concat(dataframes_list, ignore_index=True)\n",
    "\n",
    "# Write output to file\n",
    "output_df.to_csv('C:/com748/code/com748/data/processed/day_minus_0/failures.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to retrieve day minus n data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day_minus_n_data(n, date, serial_number):\n",
    "\n",
    "    # Convert string to datetime\n",
    "    date_dt = datetime.strptime(date, '%Y-%m-%d')\n",
    "    # Get day minus n date\n",
    "    date_dt_minus_n = date_dt - timedelta(days=n)\n",
    "    # Convert dates back to String\n",
    "    day_minus_n = date_dt_minus_n.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Read day_minus_n file filtered by serial_number\n",
    "    try:\n",
    "        df_day_minus_n = ddf.read_parquet(f'C:/com748/data/processed/daily/{day_minus_n}.parquet', engine='pyarrow', filters=[(\"serial_number\", \"==\", f\"{serial_number}\")])\n",
    "        df_day_minus_n = df_day_minus_n.compute()\n",
    "        if len(df_day_minus_n.index) > 0: \n",
    "            return df_day_minus_n.iloc[0]\n",
    "        else: \n",
    "            return None\n",
    "    except:\n",
    "        print(f'Could not find day_minus_n file ({day_minus_n})')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day minus 1 (1 day before last operational day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find day_minus_n file (2013-12-31)\n",
      "Could not find day_minus_n file (2013-12-31)\n",
      "Could not find day_minus_n file (2013-12-31)\n",
      "Total failures where day_minus_1 data not found: 297\n"
     ]
    }
   ],
   "source": [
    "# Read day_minus_0 failures data\n",
    "df_day_minus_0 = pd.read_csv('C:/com748/code/com748/data/processed/day_minus_0/failures.csv')\n",
    "\n",
    "# iterate through day_minus_0 failures\n",
    "df_day_minus_1 = df_day_minus_0.apply(lambda row: get_day_minus_n_data(1, row['file_date'], row['serial_number']), axis=1)\n",
    "null_count = len(df_day_minus_1[df_day_minus_1['serial_number'].isna()].index)\n",
    "print(f'Total failures where day_minus_1 data not found: {null_count}')\n",
    "\n",
    "# Drop null rows\n",
    "df_day_minus_1 = df_day_minus_1[df_day_minus_1['serial_number'].notna()]\n",
    "\n",
    "# Write data to file\n",
    "df_day_minus_1.to_csv('C:/com748/code/com748/data/processed/day_minus_1/failures.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day minus 2 (2 days before last operational day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find day_minus_n file (2013-12-31)\n",
      "Could not find day_minus_n file (2013-12-31)\n",
      "Could not find day_minus_n file (2013-12-31)\n",
      "Could not find day_minus_n file (2013-12-31)\n",
      "Could not find day_minus_n file (2013-12-31)\n",
      "Could not find day_minus_n file (2013-12-31)\n",
      "Could not find day_minus_n file (2013-12-30)\n",
      "Could not find day_minus_n file (2013-12-30)\n",
      "Could not find day_minus_n file (2013-12-30)\n",
      "Total failures where day_minus_2 data not found: 309\n"
     ]
    }
   ],
   "source": [
    "# Read day_minus_0 failures data\n",
    "df_day_minus_0 = pd.read_csv('C:/com748/code/com748/data/processed/day_minus_0/failures.csv')\n",
    "\n",
    "# iterate through day_minus_0 failures\n",
    "df_day_minus_2 = df_day_minus_0.apply(lambda row: get_day_minus_n_data(2, row['file_date'], row['serial_number']), axis=1)\n",
    "null_count = len(df_day_minus_2[df_day_minus_2['serial_number'].isna()].index)\n",
    "print(f'Total failures where day_minus_2 data not found: {null_count}')\n",
    "\n",
    "# Drop null rows\n",
    "df_day_minus_2 = df_day_minus_2[df_day_minus_2['serial_number'].notna()]\n",
    "\n",
    "# Write data to file\n",
    "df_day_minus_2.to_csv('C:/com748/code/com748/data/processed/day_minus_2/failures.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day minus 7 (7 days before last operational day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find day_minus_n file (2013-12-31)\n",
      "Could not find day_minus_n file (2013-12-31)\n",
      "Could not find day_minus_n file (2013-12-31)\n",
      "Could not find day_minus_n file (2013-12-31)\n",
      "Could not find day_minus_n file (2013-12-31)\n",
      "Could not find day_minus_n file (2013-12-31)\n",
      "Could not find day_minus_n file (2013-12-26)\n",
      "Could not find day_minus_n file (2013-12-26)\n",
      "Could not find day_minus_n file (2013-12-26)\n",
      "Could not find day_minus_n file (2013-12-26)\n",
      "Could not find day_minus_n file (2013-12-26)\n",
      "Could not find day_minus_n file (2013-12-26)\n",
      "Could not find day_minus_n file (2013-12-25)\n",
      "Could not find day_minus_n file (2013-12-25)\n",
      "Could not find day_minus_n file (2013-12-25)\n",
      "Total failures where day_minus_7 data not found: 255\n"
     ]
    }
   ],
   "source": [
    "# Read day_minus_0 failures data\n",
    "df_day_minus_0 = pd.read_csv('C:/com748/code/com748/data/processed/day_minus_0/failures.csv')\n",
    "\n",
    "# iterate through day_minus_0 failures\n",
    "df_day_minus_7 = df_day_minus_0.apply(lambda row: get_day_minus_n_data(7, row['file_date'], row['serial_number']), axis=1)\n",
    "null_count = len(df_day_minus_7[df_day_minus_7['serial_number'].isna()].index)\n",
    "print(f'Total failures where day_minus_7 data not found: {null_count}')\n",
    "\n",
    "# Drop null rows\n",
    "df_day_minus_7 = df_day_minus_7[df_day_minus_7['serial_number'].notna()]\n",
    "\n",
    "# Write data to file\n",
    "df_day_minus_7.to_csv('C:/com748/code/com748/data/processed/day_minus_7/failures.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
